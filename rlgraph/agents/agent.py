# Copyright 2018/2019 The RLgraph authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

from __future__ import absolute_import, division, print_function

from collections import defaultdict
from functools import partial
import logging

import numpy as np

from rlgraph import get_backend
from rlgraph.graphs.graph_builder import GraphBuilder
from rlgraph.graphs.graph_executor import GraphExecutor
from rlgraph.spaces import Space, ContainerSpace
from rlgraph.utils.input_parsing import parse_execution_spec
from rlgraph.utils.ops import flatten_op
from rlgraph.utils.rlgraph_errors import RLGraphError
from rlgraph.utils.specifiable import Specifiable
from rlgraph.utils.util import strip_list


# TODO: Make Agent a child of Learner.
class Agent(Specifiable):
    """
    Generic agent defining RLGraph-API operations and parses and sanitizes configuration specs.
    """
    def __init__(self, state_space, action_space, *,
                 python_buffer_size=0, custom_python_buffers=None,
                 internal_states_space=None, execution_spec=None,
                 update_spec=None, observe_spec=None, max_timesteps=None, summary_spec=None, saver_spec=None,
                 name="agent"):
        """
        Args:
            state_space (Union[dict,Space]): Spec dict for the state Space or a direct Space object.
            action_space (Union[dict,Space]): Spec dict for the action Space or a direct Space object.

            python_buffer_size (int): If > 0, will buffer observations from env on python side and only call
                `self._observe_graph` once the buffer is full or an episode has terminated. 0 for no buffering.

            custom_python_buffers (Optional[Dict[str,Space]]): An optional dict of str + Space pairs used to generate
                custom buffers (other than the ones generated by default for states, actions, rewards, etc.

            internal_states_space (Optional[Union[dict,Space]]): Spec dict for the internal-states Space or a direct
                Space object for the Space(s) of the internal (RNN) states.

            execution_spec (Optional[dict,Execution]): The spec-dict specifying execution settings.
            observe_spec (Optional[dict]): Obsoleted: All done now via `python_buffer_size`.
            update_spec (Optional[dict]): Obsoleted: Logic moved primarily into Workers.
            max_timesteps (Optional[int]): An optional max timesteps hint for Workers.
            summary_spec (Optional[dict]): Spec-dict to specify summary settings.
            saver_spec (Optional[dict]): Spec-dict to specify saver settings.
            name (str): Some name for this Agent object.
        """
        # Update_spec and observe_spec are obsoleted.
        if update_spec is not None:
            raise RLGraphError(
                "\n`update_spec` has been obsoleted!\nUse the `UpdateRules` class instead.\n"
                "The following translations need to be done:\n"
                "do_updates -> UpdateRules.do_updates\n"
                "update_mode -> UpdateRules.unit (one of `time_steps` or `episodes`)\n"
                "update_interval -> UpdateRules.update_every_n_units\n"
                "update_steps -> UpdateRules.update_repeats\n"
                "steps_before_update -> UpdateRules.first_update_after_n_units\n"
                "sync_interval -> Use `SyncRules` class, property: `SyncRules.sync_every_n_updates` and pass this "
                "SyncRules object (or a spec-dict thereof) directly into the Agent's ctor (`sync_rules` arg)\n"
            )
        if observe_spec is not None:
            raise RLGraphError(
                "`observe_spec` has been obsoleted! Use `python_buffer_size` > 0 to enable python-side experience "
                "buffering and to set the buffer's size. Use `python_buffer_size=0` to disable buffering."
            )

        super(Agent, self).__init__()
        self.name = name
        self.graph_built = False
        self.build_stats = None
        self.logger = logging.getLogger(__name__)

        self.state_space = Space.from_spec(state_space).with_batch_rank(False)
        self.preprocessed_state_space = None  # type: Space

        self.flat_state_space = self.state_space.flatten(scope_separator_at_start=False)\
            if isinstance(self.state_space, ContainerSpace) else None
        self.logger.info("Parsed state space definition: {}".format(self.state_space))
        self.action_space = Space.from_spec(action_space).with_batch_rank(False)
        self.flat_action_space = self.action_space.flatten() if isinstance(self.action_space, ContainerSpace) else None
        self.logger.info("Parsed action space definition: {}".format(self.action_space))

        # Define the input-Spaces:
        # Tag the input-Space to `self.set_weights` as equal to whatever the variables-Space will be for
        # the Agent's policy Component.
        self.input_spaces = dict(
            states=self.state_space.with_batch_rank(),
            time_percentage=float
        )
        self.internal_states_space = Space.from_spec(internal_states_space)

        # Global time step counter.
        self.timesteps = 0
        # Global updates counter.
        self.num_updates = 0
        # An optional maximum timestep value to use by Workers to figure out `time_percentage` in calls to
        # `update()` and `get_action()`.
        self.max_timesteps = max_timesteps

        # TODO: Create spec class for execution spec.
        self.execution_spec = parse_execution_spec(execution_spec)

        # Python-side buffering enabled and how large?
        self.python_buffer_size = python_buffer_size

        # Python-side experience buffer for better performance (may be disabled).
        self.default_env = "env_0"

        # Generate all our python buffers to collect data before it goes into the graph.
        def factory_(i):
            if i == 0:
                return []
            return tuple([[] for _ in range(i)])

        self.states_buffer = defaultdict(partial(factory_, len(self.flat_state_space or [])))
        self.actions_buffer = defaultdict(partial(factory_, len(self.flat_action_space or [])))
        self.internals_buffer = defaultdict(list)
        self.rewards_buffer = defaultdict(list)
        self.next_states_buffer = defaultdict(partial(factory_, len(self.flat_state_space or [])))
        self.terminals_buffer = defaultdict(list)
        self.sequence_indices_buffer = defaultdict(list)
        self.custom_buffers = {}
        self.custom_buffer_spaces = {}
        if custom_python_buffers is not None:
            for key, space in custom_python_buffers.items():
                flat_space = Space.from_spec(space).flatten()
                self.custom_buffer_spaces[key] = None if len(flat_space) == 1 else flat_space
                self.custom_buffers[key] = defaultdict(partial(factory_, len(flat_space)))

        # The Agent's root-Component.
        self.root_component = None

        # Create our GraphBuilder and -Executor.
        self.graph_builder = GraphBuilder(action_space=self.action_space, summary_spec=summary_spec)
        self.graph_executor = GraphExecutor.from_spec(
            get_backend(),
            graph_builder=self.graph_builder,
            execution_spec=self.execution_spec,
            saver_spec=saver_spec
        )  # type: GraphExecutor

    def reset_env_buffers(self, env_id=None):
        """
        Resets all environment buffers for buffered `observe` calls.

        Args:
            env_id (Optional[str]): Environment id to reset. Defaults to a default environment if None provided.
        """
        if env_id is None:
            env_id = self.default_env

        del self.states_buffer[env_id]
        del self.actions_buffer[env_id]
        del self.internals_buffer[env_id]
        del self.rewards_buffer[env_id]
        del self.next_states_buffer[env_id]
        del self.terminals_buffer[env_id]
        del self.sequence_indices_buffer[env_id]
        for key, buffer in self.custom_buffers.items():
            del self.custom_buffers[key][env_id]

    def build(self, build_options=None):
        """
        Builds this agent. This method call only be called if the agent parameter "auto_build"
        was set to False.

        Args:
            build_options (Optional[dict]): Optional build options, see build doc.
        """
        assert not self.graph_built, \
            "ERROR: Attempting to build agent which has already been built. Ensure `auto_build` c'tor arg is set to " \
            "False and the `Agent.build` method has not been called twice."

        self.build_stats = self.graph_executor.build(
            root_components=[self.root_component], input_spaces=self.input_spaces,
            build_options=build_options, batch_size=self.root_component.memory_batch_size
        )

        self.graph_built = True
        return self.build_stats

    def preprocess_states(self, states):
        """
        Applies the agent's preprocessor to one or more states, e.g. to preprocess external data
        before inserting to memory without acting. Returns identity if no preprocessor defined.

        Args:
            states (np.array): State(s) to preprocess.

        Returns:
            np.array: Preprocessed states.
        """
        if self.root_component.preprocessing_required:
            return self.graph_executor.execute(("preprocess_states", states))
        else:
            # Return identity.
            return states

    def get_action(self, states, internals=None, use_exploration=True, apply_preprocessing=True, extra_returns=None,
                   time_percentage=None):
        """
        Returns action(s) for the passed state(s). If `states` is a single state, returns a single action, otherwise,
        returns a batch of actions, where batch-size = number of states passed in.

        Args:
            states (Union[dict,np.ndarray]): States dict/tuple or numpy array.
            internals (Union[dict,np.ndarray]): Internal states dict/tuple or numpy array.

            use_exploration (bool): If False, no exploration or sampling may be applied
                when retrieving an action.

            apply_preprocessing (bool): If True, apply any state preprocessors configured to the action. Set to
                false if all pre-processing is handled externally both for acting and updating.

            extra_returns (Optional[List[str],str]): Optional string or list of strings for additional return
                values (besides the actions). Possible values are:
                - 'preprocessed_states': The preprocessed states after passing the given states through the
                preprocessor stack.
                - 'internal_states': The internal states returned by the RNNs in the NN pipeline.
                - 'used_exploration': Whether epsilon- or noise-based exploration was used or not.

        Returns:
            tuple or single value depending on `extra_returns`:
                - Action(s) as dict/tuple/np.ndarray (depending on `self.action_space`).
                - (Optional): The preprocessed states.
        """
        if time_percentage is None:
            time_percentage = self.timesteps / (self.max_timesteps or 1e6)

        extra_returns = [extra_returns] if isinstance(extra_returns, str) else (extra_returns or [])
        # States come in without preprocessing -> use state space.
        if apply_preprocessing:
            call_method = "get_actions"
            batched_states, remove_batch_rank = self.state_space.force_batch(states, horizontal=False)
        else:
            call_method = "get_actions_from_preprocessed_states"
            batched_states = states
            remove_batch_rank = False

        # Increase timesteps by the batch size (number of states in batch).
        if not isinstance(batched_states, (dict, tuple)):
            batch_size = len(batched_states)
        elif isinstance(batched_states, dict):
            batch_size = len(batched_states[next(iter(batched_states))])
        else:
            batch_size = len(next(iter(batched_states)))
        self.timesteps += batch_size

        ret = self.graph_executor.execute((
            call_method,
            [batched_states, not use_exploration, time_percentage],  # deterministic = not use_exploration
            # Control, which return value to "pull" (depending on `extra_returns`).
            ["actions"] + extra_returns
        ))
        if remove_batch_rank:
            return strip_list(ret)
        else:
            return ret

    def observe(self, preprocessed_states, actions, internals, rewards, next_states, terminals,
                sequence_indices=None, env_id=None, batched=False, **other_data):
        """
        Observes an experience tuple or a batch of experience tuples. Note: If configured,
        first uses buffers and then internally calls _observe_graph() to actually run the computation graph.
        If buffering is disabled, this just routes the call to the respective `_observe_graph()` method of the
        child Agent.

        Args:
            preprocessed_states (Union[dict,ndarray]): Preprocessed states dict or array.
            actions (Union[dict,ndarray]): Actions dict or array containing actions performed for the given state(s).

            internals (Optional[list]): Internal state(s) returned by agent for the given states.Must be
                empty list if no internals available.

            rewards (Union[float,List[float]]): Scalar reward(s) observed.
            terminals (Union[bool,List[bool]]): Boolean indicating terminal.
            next_states (Union[dict,ndarray]): Preprocessed next states dict or array.

            sequence_indices (Union[bool,List[bool]): Boolean indicating an end (not an episode terminal!) of a
                consecutive episode fragment in the buffer/memory.

            env_id (Optional[str]): Environment id to observe for. When using vectorized execution and
                buffering, using environment ids is necessary to ensure correct trajectories are inserted.
                See `SingleThreadedWorker` for example usage.

            batched (bool): Whether given data (states, actions, etc..) is already batched or not.

            other_data (Optional[Dict[str,ndarray]]): Custom data that should match the structure of our custom
                buffers.
        """
        # Check for illegal internals.
        if internals is None:
            internals = []

        if self.python_buffer_size > 0:
            if env_id is None:
                env_id = self.default_env

            # If data is already batched, just have to extend our buffer lists.
            if batched:
                if self.flat_state_space is not None:
                    for i, flat_key in enumerate(self.flat_state_space.keys()):
                        self.states_buffer[env_id][i].extend(preprocessed_states[flat_key])
                        self.next_states_buffer[env_id][i].extend(next_states[flat_key])
                else:
                    self.states_buffer[env_id].extend(preprocessed_states)
                    self.next_states_buffer[env_id].extend(next_states)
                if self.flat_action_space is not None:
                    flat_action = flatten_op(actions)
                    for i, flat_key in enumerate(self.flat_action_space.keys()):
                        self.actions_buffer[env_id][i].append(flat_action[flat_key])
                else:
                    self.actions_buffer[env_id].extend(actions)
                self.internals_buffer[env_id].extend(internals)
                self.rewards_buffer[env_id].extend(rewards)
                self.terminals_buffer[env_id].extend(terminals)
                if sequence_indices is not None:
                    self.sequence_indices_buffer[env_id].extend(sequence_indices)
                for buffer_key, other_data_item in other_data.items():
                    if buffer_key in self.custom_buffer_spaces:
                        if self.custom_buffer_spaces[buffer_key] is not None:
                            for i, sub_key in enumerate(self.custom_buffer_spaces[buffer_key].keys()):
                                self.custom_buffers[buffer_key][env_id][i].extend(other_data_item[sub_key])
                        else:
                            self.custom_buffers[buffer_key][env_id].extend(other_data_item)

            # Data is not batched, append single items (without creating new lists first!) to buffer lists.
            else:
                if self.flat_state_space is not None:
                    for i, flat_key in enumerate(self.flat_state_space.keys()):
                        self.states_buffer[env_id][i].append(preprocessed_states[flat_key])
                        self.next_states_buffer[env_id][i].append(next_states[flat_key])
                else:
                    self.states_buffer[env_id].append(preprocessed_states)
                    self.next_states_buffer[env_id].append(next_states)
                if self.flat_action_space is not None:
                    flat_action = flatten_op(actions)
                    for i, flat_key in enumerate(self.flat_action_space.keys()):
                        self.actions_buffer[env_id][i].append(flat_action[flat_key])
                else:
                    self.actions_buffer[env_id].append(actions)
                self.internals_buffer[env_id].append(internals)
                self.rewards_buffer[env_id].append(rewards)
                self.terminals_buffer[env_id].append(terminals)
                if sequence_indices is not None:
                    self.sequence_indices_buffer[env_id].append(sequence_indices)
                for buffer_key, other_data_item in other_data.items():
                    if buffer_key in self.custom_buffer_spaces:
                        if self.custom_buffer_spaces[buffer_key] is not None:
                            for i, sub_key in enumerate(self.custom_buffer_spaces[buffer_key].keys()):
                                self.custom_buffers[buffer_key][env_id][i].append(other_data_item[sub_key])
                        else:
                            self.custom_buffers[buffer_key][env_id].append(other_data_item)

            buffer_is_full = len(self.rewards_buffer[env_id]) >= self.python_buffer_size

            # If the buffer (per environment) is full OR the episode was terminated. -> Flush the buffer.
            if buffer_is_full or self.terminals_buffer[env_id][-1]:
                # Set sequence index to True iff episode is not finished yet, but buffer is full.
                if buffer_is_full and not self.terminals_buffer[env_id][-1]:
                    self.sequence_indices_buffer[env_id][-1] = True

                # TODO: Apply n-step post-processing if necessary.
                if self.flat_state_space is not None:
                    states_ = {}
                    next_states_ = {}
                    for i, key in enumerate(self.flat_state_space.keys()):
                        states_[key] = np.asarray(self.states_buffer[env_id][i])
                        next_states_[key] = np.asarray(self.next_states_buffer[env_id][i])
                        # Squeeze, but do not squeeze (1,) to ().
                        if len(states_[key]) > 1:
                            states_[key] = np.squeeze(states_[key])
                            next_states_[key] = np.squeeze(next_states_[key])
                        else:
                            states_[key] = np.reshape(states_[key], (1,))
                            next_states_[key] = np.reshape(next_states_[key], (1,))
                else:
                    states_ = np.asarray(self.states_buffer[env_id])
                    next_states_ = np.asarray(self.next_states_buffer[env_id])

                if self.flat_action_space is not None:
                    actions_ = {}
                    for i, key in enumerate(self.flat_action_space.keys()):
                        actions_[key] = np.asarray(self.actions_buffer[env_id][i])
                        # Squeeze, but do not squeeze (1,) to ().
                        if len(actions_[key]) > 1:
                            actions_[key] = np.squeeze(actions_[key])
                        else:
                            actions_[key] = np.reshape(actions_[key], (1,))
                else:
                    actions_ = np.asarray(self.actions_buffer[env_id])

                data_dict = {
                    "preprocessed_states": states_,
                    "actions": actions_,
                    "internals": np.asarray(self.internals_buffer[env_id]),
                    "rewards": np.asarray(self.rewards_buffer[env_id]),
                    "next_states": next_states_,
                    "terminals": np.asarray(self.terminals_buffer[env_id]),
                    "sequence_indices": np.asarray(self.sequence_indices_buffer[env_id])
                }

                for key, sub_buffer in self.custom_buffers.items():
                    if key in self.custom_buffer_spaces:
                        if self.custom_buffer_spaces[key] is not None:
                            data = {}
                            for i, sub_key in enumerate(self.custom_buffer_spaces[key].keys()):
                                data[sub_key] = np.asarray(sub_buffer[env_id][i])
                                # Squeeze, but do not squeeze (1,) to ().
                                if len(data[sub_key]) > 1:
                                    data[sub_key] = np.squeeze(data[sub_key])
                                else:
                                    data[sub_key] = np.reshape(data[sub_key], (1,))
                        else:
                            data = np.asarray(sub_buffer[env_id])
                            # Squeeze, but do not squeeze (1,) to ().
                            if len(data) > 1:
                                data = np.squeeze(data)
                            else:
                                data = np.reshape(data, (1,))
                    else:
                        data = np.asarray(sub_buffer[env_id])
                    data_dict[key] = data
                self._observe_graph(**data_dict)
                self.reset_env_buffers(env_id)
        else:
            if not batched:
                preprocessed_states, _ = self.preprocessed_state_space.force_batch(preprocessed_states)
                next_states, _ = self.preprocessed_state_space.force_batch(next_states)
                actions, _ = self.action_space.force_batch(actions)
                rewards = [rewards]
                terminals = [terminals]
                sequence_indices = [sequence_indices]

            more_observe_data = {} if sequence_indices is None else {"sequence_indices": sequence_indices}
            self._observe_graph(
                preprocessed_states=preprocessed_states,
                actions=actions,
                internals=internals,
                rewards=rewards,
                next_states=next_states,
                terminals=terminals,
                **more_observe_data
                # TODO: what about custom data if there is no buffering?
            )

    def _observe_graph(self, preprocessed_states, actions, internals, rewards, terminals, next_states=None,
                       **kwargs):
        """
        This methods defines the actual data-inserting call to the computation graph by executing
        the respective op via the graph executor. Since this may use varied underlying
        components and API methods, every agent defines which ops it may want to call.
        The buffered observer calls this method to move data into the graph.

        Args:
            preprocessed_states (Union[dict,ndarray]): Preprocessed states dict or array.
            actions (Union[dict,ndarray]): Actions dict or array containing actions performed for the given state(s).

            internals (Union[list]): Internal state(s) returned by agent for the given states. Must be an empty list
                if no internals available.

            rewards (Union[ndarray,list,float]): Scalar reward(s) observed.
            terminals (Union[list,bool]): Boolean indicating terminal.
            next_states (Union[dict,ndarray]): Preprocessed next states dict or array.

        Keyword Args:
            Any agent-specific (optional) data to move into the graph.
        """
        raise NotImplementedError

    def update(self, batch=None, time_percentage=None, **kwargs):
        """
        Performs an update on the computation graph either via externally experience or
        by sampling from an internal memory.

        Args:
            batch (Optional[dict]): Optional external data batch to use for update. If None, the
                agent should be configured to sample internally.

            time_percentage (Optional[float]): A percentage value (between 0.0 and 1.0) of the time already passed until
                some max timesteps have been reached. This can be used by the algorithm to decay certain parameters
                (e.g. learning rate) over time.

        Returns:
            Union(list, tuple, float): The loss value calculated in this update.
        """
        raise NotImplementedError

    def import_observations(self, observations):
        """
        Bulk imports observations, potentially using device pre-fetching. Can be optionally
        implemented by agents requiring pre-training.

        Args:
            observations (dict): Dict or list of observation data.
        """
        pass

    def reset(self):
        """
        Must be implemented to define some reset behavior (before starting a new episode).
        This could include resetting the preprocessor and other Components.
        """
        pass  # optional

    def terminate(self):
        """
        Terminates the Agent, so it will no longer be usable.
        Things that need to be cleaned up should be placed into this function, e.g. closing sessions
        and other open connections.
        """
        self.graph_executor.terminate()

    def export_graph(self, filename=None):
        """
        Any algorithm defined as a full-graph, as opposed to mixed (mixed Python and graph control flow)
        should be able to export its graph for deployment.

        Args:
            filename (str): Export path. Depending on the backend, different filetypes may be required.
        """
        self.graph_executor.export_graph_definition(filename)

    def store_model(self, path=None, add_timestep=True):
        """
        Store model using the backend's check-pointing mechanism.

        Args:
            path (str): Path to model directory.

            add_timestep (bool): Indicates if current training step should be appended to exported model.
                If false, may override previous checkpoints.
        """
        self.graph_executor.store_model(path=path, add_timestep=add_timestep)

    def load_model(self, checkpoint_directory=None, checkpoint_path=None):
        """
        Loads model from specified path location using the following semantics:

        If checkpoint directory and checkpoint path are given, attempts to find `checkpoint_path` as relative path from
        `checkpoint_directory`.

        If a checkpoint directory is given but no path (e.g. because timestep of checkpoint is not known in advance),
        attempts to fetch latest check-point.

        If no directory is given, attempts to fetch checkpoint from the full absolute path `checkpoint_path'.

        Args:
            checkpoint_directory (str): Optional path to directory containing checkpoint(s).
            checkpoint_path (str): Path to specific model checkpoint.
        """
        self.graph_executor.load_model(checkpoint_directory=checkpoint_directory, checkpoint_path=checkpoint_path)

    def get_weights(self):
        """
        Returns all weights relevant for the agent's policy for syncing purposes.

        Returns:
            any: Weights and optionally weight meta data for this model.
        """
        return self.graph_executor.execute("get_weights")

    def set_weights(self, policy_weights, value_function_weights=None):
        """
        Sets policy weights of this agent, e.g. for external syncing purposes.

        Args:
            policy_weights (any): Weights and optionally meta data to update depending on the backend.
            value_function_weights (Optional[any]): Optional value function weights.

        Raises:
            ValueError if weights do not match graph weights in shapes and types.
        """
        # TODO generic *args here and specific names in specific agents?
        if value_function_weights is not None:
            return self.graph_executor.execute(("set_weights", [policy_weights, value_function_weights]))
        else:
            return self.graph_executor.execute(("set_weights", policy_weights))

    def post_process(self, batch):
        """
        Optional method to post-processes a batch if post-processing is off-loaded to workers instead of
        executed by a central learner before computing the loss.

        The post-processing function must be able to post-process batches of multiple environments
        and episodes with non-terminated fragments via sequence-indices.

        This enables efficient processing of multi-environment batches.

        Args:
            batch (dict): Batch to process. Must contain key 'sequence-indices' to describe where
                environment fragments end (even if the corresponding episode has not terminated.

        Returns:
            any: Post-processed batch.
        """
        pass

    def __repr__(self):
        """
        Returns:
            str: A short, but informative description for this Agent.
        """
        raise NotImplementedError
